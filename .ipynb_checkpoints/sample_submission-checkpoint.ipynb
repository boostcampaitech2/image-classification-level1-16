{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "# 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939c8c4-8e55-491f-bf3c-a061b3e2dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    " \n",
    "from dataset import MaskBaseDataset, BaseAugmentation, MaskSplitByProfileDataset\n",
    "from datadoubling import DataDoubling\n",
    "import train as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "# 1. Data Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4f96c-b773-4518-8bfe-4f5e580c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval' #test데이터 경로\n",
    "train_dir = '/opt/ml/input/data/train' #train데이터 경로\n",
    "\n",
    "train_csv = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_data_images = os.path.join(train_dir, 'images')\n",
    "\n",
    "print(os.getcwd())\n",
    "print(train_data_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1587dc-6380-4bf3-bc2c-7ca3a3c81200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy complete\n"
     ]
    }
   ],
   "source": [
    "instance = DataDoubling(train_data_images)\n",
    "instance.copy_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cc4aaa-eaeb-41ae-bad2-a85e2051b3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doubling completed!\n"
     ]
    }
   ],
   "source": [
    "copy_instance = DataDoubling(os.path.join(train_dir, 'images2'))\n",
    "copy_instance.data_doubling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a8b056-de09-46c0-88dd-2185d8b991de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from efficientnet_pytorch) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16445 sha256=3a36ca4f03eb197f41a0a333626138610b27229b787fb2cc44f1d93554e0d262\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/84/b9/90/25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "# 2. Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951e515d-9862-4df1-ba43-23bb2e47da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_images = os.path.join(train_dir, 'images2')\n",
    "total_dataset = MaskBaseDataset(train_data_images)\n",
    "\n",
    "inject_transform = BaseAugmentation(224,total_dataset.mean, total_dataset.std )\n",
    "\n",
    "total_dataset.set_transform(inject_transform)\n",
    "\n",
    "#print(total_dataset[3])\n",
    "#print(total_dataset.calc_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "# 3. Data Split to (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3425b0aa-b6a1-45eb-af7e-1cdf633026ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train[1] data : (tensor([[[ 1.3777,  1.2288,  1.5597,  ..., -0.5086, -0.5252, -0.5583],\n",
      "         [ 1.3942,  1.2453,  1.5597,  ..., -0.4094, -0.4259, -0.4425],\n",
      "         [ 1.4273,  1.2453,  1.5431,  ..., -0.3928, -0.3928, -0.4094],\n",
      "         ...,\n",
      "         [ 1.0302,  0.8316,  0.5834,  ..., -1.2036, -1.1540, -1.1043],\n",
      "         [ 1.2619,  1.2453,  1.2122,  ..., -1.1705, -1.0878, -1.0712],\n",
      "         [ 0.9475,  1.1129,  1.1957,  ..., -1.1540, -1.0547, -1.0547]],\n",
      "\n",
      "        [[ 1.2619,  1.1190,  1.4365,  ...,  1.1349,  1.0714,  1.0396],\n",
      "         [ 1.2778,  1.1349,  1.4365,  ...,  1.2143,  1.1666,  1.1349],\n",
      "         [ 1.3095,  1.1349,  1.4207,  ...,  1.1984,  1.1825,  1.1666],\n",
      "         ...,\n",
      "         [ 1.1825,  0.9920,  0.7856,  ..., -0.9767, -0.9291, -0.8815],\n",
      "         [ 1.3730,  1.3571,  1.3413,  ..., -0.9450, -0.8656, -0.8497],\n",
      "         [ 1.0555,  1.1984,  1.2936,  ..., -0.9291, -0.8338, -0.8338]],\n",
      "\n",
      "        [[ 1.3208,  1.1773,  1.4643,  ...,  0.9701,  0.9223,  0.8904],\n",
      "         [ 1.3368,  1.1933,  1.4643,  ...,  1.0498,  1.0179,  0.9861],\n",
      "         [ 1.3686,  1.1933,  1.4643,  ...,  1.0498,  1.0339,  1.0179],\n",
      "         ...,\n",
      "         [ 1.3527,  1.1773,  0.9701,  ..., -0.9110, -0.8631, -0.8153],\n",
      "         [ 1.4802,  1.4802,  1.4643,  ..., -0.8791, -0.7994, -0.7834],\n",
      "         [ 1.1295,  1.2889,  1.4005,  ..., -0.8631, -0.7675, -0.7675]]]), 8), 15456\n",
      "valid[1] data : (tensor([[[1.4770, 1.4770, 1.4604,  ..., 1.3777, 1.3777, 1.3777],\n",
      "         [1.4770, 1.4770, 1.4604,  ..., 1.3777, 1.3777, 1.3777],\n",
      "         [1.4770, 1.4770, 1.4604,  ..., 1.3777, 1.3777, 1.3777],\n",
      "         ...,\n",
      "         [1.0964, 1.0964, 1.0798,  ..., 0.9309, 0.8647, 0.8316],\n",
      "         [1.0798, 1.0798, 1.0798,  ..., 0.9144, 0.8647, 0.8151],\n",
      "         [1.0798, 1.0798, 1.0798,  ..., 0.9144, 0.8482, 0.7985]],\n",
      "\n",
      "        [[1.6588, 1.6588, 1.6429,  ..., 1.5794, 1.5794, 1.5794],\n",
      "         [1.6588, 1.6588, 1.6429,  ..., 1.5794, 1.5794, 1.5794],\n",
      "         [1.6588, 1.6588, 1.6429,  ..., 1.5794, 1.5794, 1.5794],\n",
      "         ...,\n",
      "         [0.9444, 0.9444, 0.9285,  ..., 0.7697, 0.7856, 0.7856],\n",
      "         [0.9285, 0.9285, 0.9285,  ..., 0.7538, 0.7856, 0.7697],\n",
      "         [0.9285, 0.9285, 0.9285,  ..., 0.7538, 0.7697, 0.7538]],\n",
      "\n",
      "        [[1.7512, 1.7512, 1.7353,  ..., 1.6237, 1.6237, 1.6237],\n",
      "         [1.7512, 1.7512, 1.7353,  ..., 1.6237, 1.6237, 1.6237],\n",
      "         [1.7512, 1.7512, 1.7353,  ..., 1.6237, 1.6237, 1.6237],\n",
      "         ...,\n",
      "         [1.0498, 1.0498, 1.0339,  ..., 0.8585, 0.9063, 0.9223],\n",
      "         [1.0339, 1.0339, 1.0339,  ..., 0.8426, 0.8904, 0.9063],\n",
      "         [1.0339, 1.0339, 1.0339,  ..., 0.8426, 0.8745, 0.8904]]]), 10), 3444\n"
     ]
    }
   ],
   "source": [
    "split_object  = MaskSplitByProfileDataset(train_data_images)\n",
    "\n",
    "inject_transform = BaseAugmentation(224,split_object.mean, split_object.std )\n",
    "split_object.set_transform(inject_transform)\n",
    "train_dataset , valid_dataset = split_object.split_dataset()\n",
    "\n",
    "print('train[1] data : {}, {}'.format(train_dataset[1], len(train_dataset)))\n",
    "print('valid[1] data : {}, {}'.format(valid_dataset[1], len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af935e5e-69aa-4120-9c9d-39e31e2e2e9a",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e46d3-ce24-4b16-ac25-c84edf405eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchensemble\n",
    "!python train.py\n",
    "#print(tr.train(train_data_images, os.path.join(os.getcwd(), 'model.py')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217bf0f-52f4-4ec1-9d83-79e59d211af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9242a-798a-46b5-b51d-1cd2e4367f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluation\n",
    "\n",
    "print(os.path.join(os.getcwd(), 'output'))\n",
    "print(test_dir)\n",
    "#evaluation(test_dir, os.path.join(os.getcwd(), 'output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a7868-a694-482b-bfdd-dcc1d7453b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
