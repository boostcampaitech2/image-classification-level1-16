{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import minmax_scale\nimport random\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomCrop,CenterCrop, RandomRotation\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_folder = '../input/cassava-leaf-disease-classification/train_images/' #훈련할 이미지들이 있는 폴더\nsamples_df = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv') #훈련할 이미지의 이름 및 각 label 데이터 로드\nsamples_df[\"filepath\"] = training_folder+samples_df[\"image_id\"] #사진을 불러오기 쉽도록 폴더와 이미지의 이름을 합쳐 경로를 생성\nsamples_df = samples_df.drop(['image_id'],axis=1) #필요없는 이미지 이름을 모두 버림","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#이미지 파일을 가져오기 쉽게 파일이름+이미지이름을 합해서 저장\nsamples_df['filepath'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples_df = shuffle(samples_df, random_state=42) #데이터를 무작위로 섞음\ntrain_size = int(len(samples_df)*0.8) # 훈련에 사용할 데이터의 크기를 지정\ntraining_df = samples_df[:train_size] # 훈련 데이터셋을 만들어줌\nvalidation_df = samples_df[train_size:] # validation 데이터셋을 만들어줌","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8 # 배치 사이즈를 설정\nimage_size = 512 # 이미지의 크기를 설정\ninput_shape = (image_size, image_size, 3) #이미지의 사이즈 정의 (컬러 이미지이기 때문에 한 화소당 3개의 데이터가 필요)\ndropout_rate = 0.4 #드롭아웃 비율 정의\nclasses_to_predict = sorted(training_df.label.unique()) #예측해야 하는 클래스 수 정의, 여기서는 5개","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain 데이터와 validation 데이터를 텐서플로우 Dataset으로 정의합니다.\n텐서플로우 Dataset는 동적으로 데이터를 불러와, 너무 많은 데이터가 메모리에 쓰여지는 일을 방지하여 퍼포먼스가 향상됩니다.\n더 자세한 내용은 아래의 링크를 참조하세요.\nhttps://www.tensorflow.org/guide/data_performance?hl=ko\n\"\"\"\ntraining_data = tf.data.Dataset.from_tensor_slices((training_df.filepath.values, training_df.label.values))\nvalidation_data = tf.data.Dataset.from_tensor_slices((validation_df.filepath.values, validation_df.label.values))\n#텐서플로우 Dataset\n# 하나의 학습단계를 실행할 때 필요한 시간을 급격히 줄이기 위해 GPU나 TPU를 사용.\n#최대 성능을 위해서는 현재 단계가 종료되기 전에 다음 스텝의 데이터를 운반하는 효율적인 입력 파이프라인이 필요\n# tf.data api가 유연하고 효율적인 입력 파이프라인을 만드는데 도움","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image_and_label_from_path(image_path, label): #이미지 데이터를 불러와 텐서 (array와 비슷한 형태)로 변환하는 함수\n    img = tf.io.read_file(image_path) #이미지 경로의 파일을 읽음\n    img = tf.image.decode_jpeg(img, channels=3) #이미지를 array 데이터로 변환하여 저장\n    img = tf.image.random_crop(img, size=[image_size,image_size,3]) # 이미지를 랜덤으로 원하는 사이즈로 잘라줌. 중앙만 자르고 싶다면 central_crop 사용.\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE #메모리 동적 할당을 위한 AUTOTUNE\ntraining_data = training_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE) #train 데이터를 불러옴\nvalidation_data = validation_data.map(load_image_and_label_from_path,num_parallel_calls=AUTOTUNE) #validation 데이터를 불러옴","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train 및 validation 데이터를 훈련하기 좋게 batch로 자름\ntraining_data_batches = training_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\nvalidation_data_batches = validation_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n이미지를 Augumentation 해주는 레이어를 만들어줍니다. 모델을 만들 때 augmentation layer을 넣으면 자동으로 이미지를 다양하게 변환하여 줍니다.\n더 많은 augumentation을 적용해보고 싶으면 https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing 이 링크를 참조하세요.\n또한, imgaug, albumentation과 같은 강력한 augumentation 라이브러리도 살펴보세요. \n\"\"\"\ndata_augmentation_layers = tf.keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"), #랜덤으로 이미지를 좌우로 뒤집어줌.\n        layers.experimental.preprocessing.RandomRotation(0.25), #이미지를 좌우로 25% 이내로 랜덤으로 돌립니다. \n        layers.experimental.preprocessing.RandomZoom((-0.2, 0)), #이미지를 0~20%만큼 랜덤으로 축소합니다.\n        \n    ]\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델만들기 및 학습"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n이 베이스라인에서는 transfer learning을 사용합니다. 미리 훈련되어 있는 이미지용 모델을 불러와서 그 모델의 뒤쪽에 나만의 모델을 추가한 뒤 학습하는 방식입니다.\n직접 수많은 레이어의 모델을 디자인하는 것은 어렵기 때문에 이러한 방법을 사용합니다.\n여기서는 구글의 EfficientNetB0를 사용합니다. 이 모델에 대한 자세한 내용은 https://arxiv.org/pdf/1905.11946.pdf 이 논문을 참고하세요.\n\n주의!! imagenet 가중치 값을 다운받기 위하여 우측 상단 |< 표시를 누르고 setting에서 Internet을 켜줘야합니다.\n\"\"\"\nefficientnet = EfficientNetB0(weights=\"imagenet\", #이미지넷 가중치 값을 불러와 적용\n                              include_top=False, \n                              input_shape=input_shape, \n                              drop_connect_rate=dropout_rate) #efficientnetB0 모델을 로드\nefficientnet.trainable=True # efficientnetb0의 학습을 허용","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n자신만의 CNN 모델을 직접 만들어 보아도 괜찮습니다.\n\"\"\"\nmodel = Sequential() #새 Sequential 모델을 만듬 \nmodel.add(Input(shape=input_shape)) #인풋을 이미지 사이즈로 설정\nmodel.add(data_augmentation_layers) #이미지 augumentation 레이어 추가\nmodel.add(efficientnet) # efficientnetb0 추가\nmodel.add(layers.GlobalAveragePooling2D()) # 풀링 레이어를 추가\nmodel.add(layers.Dropout(dropout_rate)) # 드롭아웃 레이어를 추가\nmodel.add(Dense(len(classes_to_predict), activation=\"softmax\")) #마지막 덴스 레이어를 추가. 예측할 클래스의 개수만큼이 아웃풋이 된다. \nmodel.summary() #모델 확인","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5 #에폭 수를 설정합니다.\ndecay_steps = int(round(len(training_df)/batch_size))*epochs\ncosine_decay = CosineDecay(initial_learning_rate=1e-4, decay_steps=decay_steps, alpha=0.3) #learning rate를 에폭이 지날수록 점점 줄여나가는 cosine decay 방법을 사용합니다. \ncallbacks = [ModelCheckpoint(filepath='mymodel.h5', monitor='val_loss', save_best_only=True), #가장 validation loss가 낮은 에폭의 모델을 .h5 파일로 저장합니다. \n            EarlyStopping(monitor='val_loss', patience = 5, verbose=1)] #정해진 에폭이 되기 전에 5번의 에폭동한 validation loss가 향상되지 않으면 학습을 종료합니다. \n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(cosine_decay), metrics=[\"accuracy\"]) #loss는 sparse_categorical_crossentropy, optimizer는 Adam을 사용합니다. 각 에폭당 정확도를 통해 모델의 성능을 모니터링합니다, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(training_data_batches, #모델을 학습합니다. \n                  epochs = epochs, \n                  validation_data=validation_data_batches,\n                  callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}